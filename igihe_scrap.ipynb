{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjpS8uet6IacfNhDXd7uGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bateyjosue/NLP_Fellowship/blob/main/igihe_scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "Based on the above, get the main articles from igihe from February 2022 - present\n",
        "\n",
        "Steps to do this\n",
        "\n",
        "\n",
        "1.   Get the links to the main pages from january. Create a list\n",
        "2.   In each link, get all the links to the main articles\n",
        "3.   For each article, get the main tag that holds the texts\n",
        "4.   Get the text and store them in a txt file. The data will be used in week 2\n",
        "5.   Each article its own txt file. Naming is the date_article_1\n"
      ],
      "metadata": {
        "id": "gr0gxC2eCK6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "J1uvYjvYCWdT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAke an API Call \n",
        "def get_content(url):\n",
        "  response = requests.get(url=url)\n",
        "  content = BeautifulSoup(response.content, 'lxml')\n",
        "  return content\n",
        "\n",
        "def get_data(url, month, day):\n",
        "  response = requests.get(url+'{:02d}{:02d}'.format(month,day))\n",
        "  return response.json()\n"
      ],
      "metadata": {
        "id": "5rt_3DPqC9CS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_data('https://archive.org/wayback/available?url=igihe.com/&timestamp=2022',1,23)\n"
      ],
      "metadata": {
        "id": "EWjmUioBETSs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_snaps(data):\n",
        "  if data['archived_snapshots']:\n",
        "    time_stamp = data['timestamp'] \n",
        "    time_stamp_closest = data['archived_snapshots']['closest']['timestamp'][:8]\n",
        "    result = time_stamp == time_stamp_closest\n",
        "  else: \n",
        "    result = False\n",
        "  return result\n",
        "\n",
        "print(check_snaps(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pSas9oiKa5",
        "outputId": "009be11d-ebec-4d90-d1b3-51bae4eb4a65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.   Get the links to the main pages from january. Create a list\n",
        "def get_main_links(url):\n",
        "  links = []\n",
        "  for month in range(1, 11):\n",
        "    for day in range(1, 32):\n",
        "      data = get_data(url, month, day)\n",
        "      if check_snaps(data):\n",
        "        links.append(data['archived_snapshots']['closest']['url'])\n",
        "  return links\n",
        "\n",
        "main_links = get_main_links('https://archive.org/wayback/available?url=igihe.com/&timestamp=2022')\n",
        "  "
      ],
      "metadata": {
        "id": "mKztzr-ti3af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main = list(set(main_links))\n",
        "print(f'{len(main)} => {len(main_links)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKtGif5uoQaM",
        "outputId": "cb8d5fef-ac1c-4b99-eb2d-970783c122ee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126 =>126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.   In each link, get all the links to the main articles\n",
        "def main_articles_link(links):\n",
        "  article_links = []\n",
        "  for link in links:\n",
        "    content = get_content(link)\n",
        "    for title in content.find_all('span', class_='homenews-title'):\n",
        "      article_links.append(link + title.find('a')['href'])\n",
        "  return article_links\n",
        "\n",
        "main_articles = main_articles_link(main_links)\n"
      ],
      "metadata": {
        "id": "15U4F4g4zLjx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.   For each article, get the main tag that holds the texts\n",
        "def main_tag_text(main_articles):\n",
        "  text = ''\n",
        "  list_text = []\n",
        "  for main_article in main_articles:\n",
        "    response = get_content(main_article)\n",
        "    main_text = response.find('div', class_='fulltext margintop10')\n",
        "    if main_text != None:\n",
        "      text = ''.join([i.get_text() for i in main_text.find_all('p') if i.get_text()])\n",
        "      list_text.append(text)\n",
        "  \n",
        "  return list_text\n",
        "\n",
        "articles_text = main_tag_text(main_articles)"
      ],
      "metadata": {
        "id": "L6YnfSf8RLeV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.   Get the text and store them in a txt file. The data will be used in week 2\n",
        "open('articles_text.txt', 'w');\n",
        "with open(articles_text.txt, 'a') as files:\n",
        "  for text in main_tags_text:\n",
        "    files.writelines(text+ '\\n')"
      ],
      "metadata": {
        "id": "UmEs7I7IbRV4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(main_articles[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qTW6LNBy9NW",
        "outputId": "7703c07d-e07d-4f08-d5f1-c9795dde0537"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://web.archive.org/web/20220113045331/https://igihe.com/imikino/indi-mikino/article/djokovic-yemeye-ko-atishyize-mu-kato-kandi-arwaye-covid-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rH6Hi-ezotp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.   Each article its own txt file. Naming is the date_article_1\n",
        "import re\n",
        "\n",
        "link = main_articles[1]\n",
        "# r = requests.get(link).content\n",
        "# soup = BeautifulSoup(r, 'lxml')\n",
        "# soup.find_all('div')\n",
        "pattern = re.compile(r'\\d+')\n",
        "match = re.search(pattern, link)\n",
        "date = match.group()\n",
        "resp = requests.get(url= link)\n",
        "soups = BeautifulSoup(resp.content, 'lxml')\n",
        "div = soups.find('div', class_='fulltext margintop10')\n",
        "text = ''.join([i.get_text() for i in div.find_all('p') if i.get_text()])\n",
        "with open('{}_text_1.txt'.format(date), 'w+') as file:\n",
        "  file.write(text)\n"
      ],
      "metadata": {
        "id": "K7SxojG_j2ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWuZs8SWszOl"
      },
      "execution_count": 142,
      "outputs": []
    }
  ]
}