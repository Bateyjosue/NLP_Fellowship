{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/LO56AwDbEsTEsIBA5Q+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bateyjosue/NLP_Fellowship/blob/main/igihe_scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "Based on the above, get the main articles from igihe from February 2022 - present\n",
        "\n",
        "Steps to do this\n",
        "\n",
        "\n",
        "1.   Get the links to the main pages from january. Create a list\n",
        "2.   In each link, get all the links to the main articles\n",
        "3.   For each article, get the main tag that holds the texts\n",
        "4.   Get the text and store them in a txt file. The data will be used in week 2\n",
        "5.   Each article its own txt file. Naming is the date_article_1\n"
      ],
      "metadata": {
        "id": "gr0gxC2eCK6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "J1uvYjvYCWdT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAke an API Call \n",
        "def get_content(url):\n",
        "  response = requests.get(url=url)\n",
        "  content = BeautifulSoup(response.content, 'lxml')\n",
        "  return content\n",
        "\n",
        "def get_data(url, month, day):\n",
        "  response = requests.get(url+'{:02d}{:02d}'.format(month,day))\n",
        "  return response.json()\n"
      ],
      "metadata": {
        "id": "5rt_3DPqC9CS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_data('https://archive.org/wayback/available?url=igihe.com/&timestamp=2022',1,23)\n"
      ],
      "metadata": {
        "id": "EWjmUioBETSs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_snaps(data):\n",
        "  if data['archived_snapshots']:\n",
        "    time_stamp = data['timestamp'] \n",
        "    time_stamp_closest = data['archived_snapshots']['closest']['timestamp'][:8]\n",
        "    result = time_stamp == time_stamp_closest\n",
        "  else: \n",
        "    result = False\n",
        "  return result\n",
        "\n",
        "print(check_snaps(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pSas9oiKa5",
        "outputId": "6a23490f-3daa-4ee0-b47e-19e13b417794"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.   Get the links to the main pages from january. Create a list\n",
        "def get_main_links(url):\n",
        "  links = []\n",
        "  for month in range(1, 11):\n",
        "    for day in range(1, 32):\n",
        "      data = get_data(url, month, day)\n",
        "      if check_snaps(data):\n",
        "        links.append(data['archived_snapshots']['closest']['url'])\n",
        "  return links\n",
        "\n",
        "main_links = get_main_links('https://archive.org/wayback/available?url=igihe.com/&timestamp=2022')\n",
        "  "
      ],
      "metadata": {
        "id": "mKztzr-ti3af"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.   In each link, get all the links to the main articles\n",
        "def main_articles_link(links):\n",
        "  article_links = []\n",
        "  for link in links:\n",
        "    content = get_content(link)\n",
        "    for title in content.find_all('span', class_='homenews-title'):\n",
        "      article_links.append(link + title.find('a')['href'])\n",
        "  return article_links\n",
        "\n",
        "main_articles = main_articles_link(main_links[:2])\n"
      ],
      "metadata": {
        "id": "15U4F4g4zLjx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.   For each article, get the main tag that holds the texts\n",
        "def main_tag_text(main_articles):\n",
        "  text = ''\n",
        "  list_text = []\n",
        "  for main_article in main_articles:\n",
        "    response = get_content(main_article)\n",
        "    main_text = response.find('div', class_='fulltext margintop10')\n",
        "    if main_text != None:\n",
        "      text = ''.join([i.get_text() for i in main_text.find_all('p') if i.get_text()])\n",
        "      list_text.append(text)\n",
        "  \n",
        "  return list_text\n",
        "\n",
        "articles_text = main_tag_text(main_articles)"
      ],
      "metadata": {
        "id": "L6YnfSf8RLeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.   Get the text and store them in a txt file. The data will be used in week 2\n",
        "def get_text_from_link(url):\n",
        "  articles = []\n",
        "  titles = []\n",
        "  for link in url:\n",
        "    soup = get_content(link)\n",
        "    articles.append(soup.find('div', class_='fulltext margintop10').get_text())\n",
        "    with open('articles_text.txt', 'a+') as file:\n",
        "      file.writelines(soup.find('div', class_='fulltext margintop10').get_text())\n",
        "    # titles.append(soup.find('h3', class_='title-article'))\n",
        "\n",
        "get_text_from_link(main_articles)"
      ],
      "metadata": {
        "id": "UmEs7I7IbRV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.   Each article its own txt file. Naming is the date_article_1\n",
        "import re\n",
        "\n",
        "link = main_articles[1]\n",
        "# r = requests.get(link).content\n",
        "# soup = BeautifulSoup(r, 'lxml')\n",
        "# soup.find_all('div')\n",
        "pattern = re.compile(r'\\d+')\n",
        "match = re.search(pattern, link)\n",
        "date = match.group()\n",
        "resp = requests.get(url= link)\n",
        "soups = BeautifulSoup(resp.content, 'lxml')\n",
        "div = soups.find('div', class_='fulltext margintop10')\n",
        "text = ''.join([i.get_text() for i in div.find_all('p') if i.get_text()])\n",
        "with open('{}_text_1.txt'.format(date), 'w+') as file:\n",
        "  file.write(text)\n"
      ],
      "metadata": {
        "id": "K7SxojG_j2ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = get_content('https://www.microverse.org/')"
      ],
      "metadata": {
        "id": "hWuZs8SWszOl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimonials = content.find_all('div', class_='orig_featured_testimonial_v2')"
      ],
      "metadata": {
        "id": "d5wpK1qeNP8q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for testimonials in testimonials:\n",
        "  print(testimonials.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH9cEeA6NWtT",
        "outputId": "697a8fd7-7df4-4d81-f678-8a238aab6587"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â€” \"For me, it was a no-brainer. I wanted to try Microverse because while the alternatives would give you a tutorial or a course, you have to do it alone, but with this, I had the accountability.\"â€œKevin M, Senior Software Developer at MicrosoftðŸ‡°ðŸ‡ª Kenya\n",
            "â€” \"My life has changed 100% since Microverse. My current salary is 5.6 times more per year than my previous salary.\"â€œGabriela C, Frontend Developer at Upstart 13ðŸ‡²ðŸ‡½ Mexico\n",
            "â€” \"To be successful I knew I had to develop skills in planning, discipline, and productivity. Microverse provided the structure so I could grow in these areas, which prepared me for working in the software industry.\"â€œAlex S, Full-Stack Developer at DoctolibðŸ‡©ðŸ‡ª Germany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zLkfMRqN04p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}